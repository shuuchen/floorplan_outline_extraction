{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from res_unet import ResUNet\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.nn import Module\n",
    "from torch.optim import Adam\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 補助関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img1, img2):\n",
    "    \n",
    "    img1 = TF.to_pil_image(img1, mode='L')\n",
    "    img2 = TF.to_pil_image(img2, mode='L')\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img1)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(img2)\n",
    "    \n",
    "    plt.show()    \n",
    "    \n",
    "def show_plot(train_loss, val_loss):\n",
    "    plt.plot(train_loss, label='train_loss')\n",
    "    plt.plot(val_loss, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = './data/image'\n",
    "label_dir = './data/outline'\n",
    "checkpoint_dir = './checkpoint'\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "train_file = './data/train.txt'\n",
    "val_file = './data/val.txt'\n",
    "test_file = './data/test.txt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MadoriOutlineDS(Dataset):\n",
    "    \n",
    "    def _prepare(self, data_file):\n",
    "        with open(data_file) as f:\n",
    "            lines = f.readlines()\n",
    "        f.close()\n",
    "        self.img_list = []\n",
    "        self.label_list = []\n",
    "        for line in lines:\n",
    "            f_name = line.strip()\n",
    "            self.img_list += [os.path.join(img_dir, f'{f_name}.jpg')]\n",
    "            self.label_list += [os.path.join(label_dir, f'{f_name}.png')]\n",
    "    \n",
    "    def __init__(self, data_file, img_size=(256, 256)):\n",
    "        self._prepare(data_file)\n",
    "        self.img_size = (256, 256)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def _transform(self, img, label):\n",
    "        img = img.resize(self.img_size, Image.BILINEAR)\n",
    "        label = label.resize(self.img_size, Image.BILINEAR)\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            rot_degree = random.choice([90, 180, 270])\n",
    "            img = TF.rotate(img, rot_degree)\n",
    "            label = TF.rotate(label, rot_degree)\n",
    "        if random.random() > 0.5:\n",
    "            img = TF.hflip(img)\n",
    "            label = TF.hflip(label)\n",
    "        if random.random() > 0.5:\n",
    "            img = TF.vflip(img)\n",
    "            label = TF.vflip(label)\n",
    "        \n",
    "        return TF.to_tensor(img), TF.to_tensor(label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_list[idx]).convert('L')\n",
    "        label = Image.open(self.label_list[idx]).convert('L')\n",
    "        return self._transform(img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(MadoriOutlineDS(train_file), batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(MadoriOutlineDS(val_file), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i, batch in enumerate(train_dl):\n",
    "    img, label = batch\n",
    "    print(img.size(), label.size())\n",
    "    for k in range(batch_size):\n",
    "        imshow(img[k], label[k])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice loss\n",
    "- https://github.com/kevinzakka/pytorch-goodies/blob/master/losses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        \n",
    "    def forward(true, logits, eps=1e-7):\n",
    "        num_classes = logits.shape[1]\n",
    "        if num_classes == 1:\n",
    "            true_1_hot = torch.eye(num_classes + 1)[true.squeeze(1)]\n",
    "            true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "            true_1_hot_f = true_1_hot[:, 0:1, :, :]\n",
    "            true_1_hot_s = true_1_hot[:, 1:2, :, :]\n",
    "            true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n",
    "            pos_prob = torch.sigmoid(logits)\n",
    "            neg_prob = 1 - pos_prob\n",
    "            probas = torch.cat([pos_prob, neg_prob], dim=1)\n",
    "        else:\n",
    "            true_1_hot = torch.eye(num_classes)[true.squeeze(1)]\n",
    "            true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "            probas = F.softmax(logits, dim=1)\n",
    "        true_1_hot = true_1_hot.type(logits.type())\n",
    "        dims = (0,) + tuple(range(2, true.ndimension()))\n",
    "        intersection = torch.sum(probas * true_1_hot, dims)\n",
    "        cardinality = torch.sum(probas + true_1_hot, dims)\n",
    "        dice_loss = (2. * intersection / (cardinality + eps)).mean()\n",
    "        return (1 - dice_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResUNet().to(device)\n",
    "criterion = DiceLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history, val_loss_history = [], []\n",
    "lowest_epoch_train_loss = lowest_epoch_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        img, label = batch\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(img)\n",
    "        batch_train_loss = criterion(label, pred)\n",
    "        epoch_train_loss += batch_train_loss.item()\n",
    "        batch_train_loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_train_loss /= (i+1)\n",
    "    if epoch_train_loss < lowest_epoch_train_loss:\n",
    "        lowest_epoch_train_loss = epoch_train_loss\n",
    "        torch.save(model.state_dict(), f'{checkpoint_dir}/best_train.pth')\n",
    "    train_loss_history += [epoch_train_loss]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss = 0\n",
    "        for i, batch in enumerate(val_dl):\n",
    "            img, label = batch\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            pred = model(img)\n",
    "            batch_val_loss = criterion(label, pred)\n",
    "            epoch_val_loss += batch_val_loss.item()\n",
    "        epoch_val_loss /= (i+1)\n",
    "        if epoch_val_loss < lowest_epoch_val_loss:\n",
    "            lowest_epoch_val_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), f'{checkpoint_dir}/best_val.pth')\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "    print(f'Epoch {epoch} training loss is {epoch_train_loss}, validation loss is {epoch_val_loss}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(train_loss_history, val_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResUNet().to(device)\n",
    "model.load_state_dict(torch.load(f'{checkpoint_dir}/best_val.pth'))\n",
    "test_dl = DataLoader(MadoriOutlineDS(test_file), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_results_0 = []\n",
    "test_results_1 = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_dl):\n",
    "        if batch_no > 10: break\n",
    "        img, label = batch\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        pred = model(img)\n",
    "        \n",
    "        img, label, pred = img.cpu(), label.cpu(), pred.cpu()\n",
    "        \n",
    "        imshow(label[0], pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
